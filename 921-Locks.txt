presence of interrupts on a single process(or multiple thread excuting on multiple processors concurently).
locks, programmers annote üçî source code with locks, putting them around critical sections, 
thus ensure that any such critical section excutes as if it were a single atomic instruction.


28.1 Locks: The Basic Idea

updata of a shared variable:
balance = balance + 1;

Of course, other critical sections are possible, such as adding an element to a linked list or other more complex updates to shared structures,

To use a lock, we add some code around the critical section like this:
lock_t mutex; // some globally-allocated lock ‚Äômutex‚Äô
...
lock(&mutex);
balance = balance + 1;
unlock(&mutex);

a lock is just a variable, and to use one, you must declare a lock variable(e.g..mutex above).
lock holds tha state of the lock at any instant in time, 1Ô∏è‚É£ available,(unlocked or free) no thread holds the lock,
or 2Ô∏è‚É£ acquired (locked or held), and thus exactly one thread hold the lock and is in a critical section.

we could store other info. in data type as well, 
1Ô∏è‚É£  which thread hold the lock, a queue for ordering lock acquisition, 
2Ô∏è‚É£  all is hidden from the user of the lock.

The semantics of the lock() and unlock() routines are simple.
calling lock() routine tries to acquire the lock; if no other thread hold the lock(i.e it is free), thread will acquire the lock,
entring the critical section; thread this called the owner of the lock.

if another thread then call lock() with same variable mutex, it will not return while the lock is held by another thread;
‚≠ïin this way, other threads are prevented from entering the critical section while the first thread that holds the lock is in there. 

once the owner calls unlock() the lock is available again.
1Ô∏è‚É£ no other thread waiting for the lock, state of the lock simply changed to free.(no thread has called lock() an stuck waiting).
2Ô∏è‚É£ if there are waiting threads (stuck in lock()), one of them will (eventually)  notice (or be informed of) this change of lock's state,       acqurire the lock, enter critical section.


Locks provide some minimal amount of control over scheduling to programmers.
threads created by programmer but scheduled by the OS, in any fashion OS choose.
Locks yield some of that control back to the programmer;
the programmer can guarantee that no more than a single thread can ever be active within that code.
Thus locks help transform the chaos that is traditional OS scheduling into a more controlled activity.
ÿßŸÑÿ´ÿ±ÿØÿ≤ ÿßŸÑÿ®ÿ±Ÿàÿ¨ÿ±ÿßŸÖÿ± ŸáŸà ÿßŸÑŸÑŸä ÿ®ŸäÿπŸÖŸÑŸáÿß ÿ®ÿ≥ ÿ®Ÿäÿ™ŸÖ ÿ™ÿ±ÿ™ÿ®Ÿáÿß ÿ≠ÿ≥ÿ® ŸÜÿ∏ÿßŸÖ ÿßŸÑÿ™ÿ¥ÿ∫ŸäŸÑ
ÿßŸÑÿßŸÇŸÅÿßŸÑ ÿ®ÿ™ÿ∂ŸäŸÇ ŸÅŸäÿ™ÿ¥Ÿäÿ±ÿ≤ ÿßŸÜ ÿßŸÑÿ®ÿ±Ÿàÿ¨ÿ±ÿßŸÖÿ± ŸáŸà ÿßŸÑŸÑŸä ŸáŸäÿπŸÖŸÑŸáÿß ŸàŸäŸÜÿ∏ŸÖŸáÿß ŸÉŸÖÿßŸÜ ÿ®ÿ≠Ÿäÿ´ ŸÖŸÅŸäÿ¥ ÿ∫Ÿäÿ± ÿ´ÿ±ŸäÿØ Ÿàÿßÿ≠ÿØÿ© ŸáŸä ÿßŸÑŸÑŸä ÿ™ÿ¥ÿ™ÿ∫ŸÑ
Ÿàÿ®ÿØŸÑ ÿßŸÑŸÅŸàÿ∂Ÿä ÿ®ÿ™ÿßÿπ ÿßŸÑÿßÿ≥ŸÉŸäÿØŸàÿßŸÑÿ± ÿ®ÿ™ÿßÿπ ÿßŸÑŸÜÿ∏ÿßŸÖ ŸáŸäÿ®ŸÇŸä ÿßŸÉÿ™ÿ± ÿ™ÿ≠ŸÉŸÖ ÿ®Ÿàÿßÿ≥ÿ∑ÿ© ÿßŸÑÿ®ÿ±Ÿàÿ¨ÿ±ÿßŸÖÿ±

28.2 Pthread Locks

the name POSIX library uses for lock is a mutux, used to provide mutual exclusion between threads,
i.e., any thread in critical section, it exclude the others from entering until it has completed the section.
(we use our wrappers  that check for errors upon lock and unlock):

  pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;
  Pthread_mutex_lock(&lock); // wrapper for pthread_mutex_lock()
  balance = balance + 1;
  Pthread_mutex_unlock(&lock);

we can use different locks to protect different variables, increaing concurrency:
1Ô∏è‚É£ instead of one big lock that is used any time any  critical section is accessed
(a coarse-grained locking strategy),the whole package is üîí .

2Ô∏è‚É£ one will often protect different data and data structures with different locks, thus allowing more threads to be in locked code at once
(a more fine-grained approach).


28.3 Building A Lock
by now you know how lock works, but how should we build a lock?
what hardware support is needed? what OS support? how we cam build efficient lock? provide mutex at low cost?

a number of different hardware primitives have been added to the instruction sets of various computer architectures;
we don't study how implement, but we study how to use them in order to build a mutual exclusion primitive like lock.
we need to study how OS gets involved to complete the picture and enable us to build a sophisticated locking library.


28.4 Evaluating Locks
befor building any locks, we should first understand what  our goals are, and ask how to evaluate the efficacy of particular 
lock implementation.

wheather the lock deos the basic task, 
which is provide ((mutual exclusion)), does the lock work ((preventing multiple threads from entering a critical section ))?

fairness,does each thread contending for the lock get a fair shot at acquiring it once it is free?
	does any thread contending for the lock starve while doing so, thus never obtaining it?

performance, time overheads added by using lock.
no contention; single thead is running grabs and releases the lock, what overhead of doing so?
multiple threads are contending for the lock on a single CPU;
muliple CPUs involved, and threads on each contending for the lock? 
we can better understand the performance impact of using various locking techniques, as described below.


28.5 Controlling Interrupts

solution earliest solutions used to provide mutual exclusion was disable interrupts for critical sections; 
for single-processor systems.
code:
void lock() {
   DisableInterrupts();
}
void unlock() {
   EnableInterrupts();
}

by turning off interrupts (using some kind of special hardware instruction) before entering a critical section, 
we ensure that code inside critical section will not be interrupted, and will excuted as if it were atomic.(grab lock)
when we finished, we re-enable interrupts (again via a hardware instruction), and thus the program proceeds as usual.(release lock)

positive of this approach is its simplicity.
with no interrupts, a thead ensure that the code it excuted will excute and no other thread will interface with it.

The negatives, unfortunately, are many
1. requires us to allow any calling thread to perform a privileged operation (turning interrupts on and off) and thus trust that  this    facility is not abused.
 . any time we are required to trust an arbitrary program, we are probably in trouble,
   gready program could call lock() at the beginning of its excution, thus monopolize the processor;
   worse, errant or malicious program could call lock() and go into endless loop.
 . OS never gain control again, restart system as solution.
   Using interrupt disabling, requires toomuch trust in applications.

2. the approach does not work on multiprocessors.
   If multiple threads are running on different CPUs, and each try to enter the same critical section, 
   it does not matter whether interrupts are disabled; threads will be able to run on other processors, and thus
   could enter critical section.

3. this aproach can be inefficient.
   masks or unmasks interrupts tends to be excuted slowly by modern CPUs.

for these reasons, turning off interrupts is only used in limited context as mutual exclusion primitive.
for example, OS use interrupt masking to guarantee atomicity when accessing its own data structures, 
or prevent certain messy interrupt handling situation from arising.
OS trust it self to perform privilaged operations anyhow. 

ASIDE: DEKKER‚ÄôS AND PETERSON‚ÄôS ALGORITHMS
Unlike the solutions we discuss here, which use special hardware instructions and even OS support	
Dekker‚Äôs approach uses just loads and stores (assuming they are atomic with respect to each other, which was true on early hardware).

just loads and stores are used, and the idea is to ensure that two threads never enter a critical section at the same time.
int flag[2];
int turn;
void init() {
	flag[0] = flag[1] = 0; // 1->thread wants to grab lock
	turn = 0; // whose turn? (thread 0 or 1?)
}
void lock() {
	flag[self] = 1; // self: thread ID of caller
	turn = 1 - self; // make it other thread‚Äôs turn
	while ((flag[1 - self] == 1) && (turn == 1 - self))
		; // spin-wait
}
void unlock() {
	flag[self] = 0; // simply undo your intent
}


28.6 Test And Set (Atomic Exchange)
disabling interrupts does not work with multiple processors, start to invent hardware support for locking.
		typedef struct __lock_t { int flag; } lock_t;

		void init(lock_t * mutex) {
		 	// 0 -> lock is available, 1 -> held
	 		mutex->flag = 0;
	
		}

		 void lock(lock_t* mutex) {
	 		while (mutex->flag == 1) // TEST the flag
		 		; // spin-wait (do nothing)
	 		mutex->flag = 1; // now SET it!

		 }

 		void unlock(lock_t* mutex) {
			 mutex->flag = 0
 		}
;---------------------------------------------------------------First Attempt: A Simple Flag

the simplest bit of hardware support to understand is what is known as a test-and-set instruction, atomic exchange.
we use simple flag variable to donate whether the lock is held or not.

in first attempt, idea quite simple: use a simple variable to indicate whether some thread has possession of a lock().

senario 1:
the first thread enters the critical section will call lock(), which tests whether the flag is equal to 1 ?(it is not, in this case),
and sets the flag to 1 to indicate that the thread now holds the lock.

finished with critical section the thread calls unlock() and clears the flag, that indicating tha lock no longer held.

senario 2:
another thread 	happens to call lock(), while that first thread in critical section, it simply spin-wait in while loop until the thread
call unlock() and clear the flag. once the first thread done so, waiting one will set the flag after get out of while loop,
and proceed into critical section.


two problems
1- correctness
2- performance
1Ô∏è‚É£  correctness problem is simple to see once you get used to thinking about concurrent programming (assuming flag = 0 to begin)

		thread 1				thread2
		---------------------------------------------------
		call lock()
		while(flag == 1)
		interrupt: switch to thread 2
							call lock()
							while(flag == 1)
							flag = 1; //take it for it self!!
							interrupt: switch to thread 1
		flag = 1 //take it too!
;--------------------------------------Table 28.1: Trace: No Mutual Exclusion


where both threads set their flags to 1 and both threads are thus able to enter the critical section. This is bad! We
have obviously failed to provide the most basic requirement: providing mutual exclusion.

2Ô∏è‚É£ the performance problem, thread waits to acquire a lock that is already held: checking the value of flag, technique known as spin-   	waiting. and that  technique wastes timewaiting for another thread to release a lock.

3Ô∏è‚É£ on uniporcessor, the wast is high, where the thread that the waiter is waiting for can not even run, so moving forward and develop more 
sophisticated solution,we should also consider ways to avoid this kind of waste.


28.7 Building A Working Spin Lock
it is not possible to implement without some support from the hardware.
some systems provide an instruction to support the creation of simple locks based on this concept.
more powerful instruction  it is the load/store unsigned byte instruction (ldstub), whereas on x86,

xchg instruction but basically does the same thing across platforms, referred to as test-and-set.


return old valure pointed by ptr, and update said value to new. the sequance of operation is performed atomically.
call test-and-set enables  you to test the old value, and setting the memory location to new value.

more powerful instruction is enough to build a simple spin lock, as we now examine in Figure 28.2

int TestAndSet(int* ptr, int new) {
	 int old = *ptr; // fetch old value at ptr
	 * ptr = new; // store ‚Äônew‚Äô into ptr
	 return old; // return the old value
	
}
typedef struct __lock_t {
	 int flag;
	
} lock_t;

void init(lock_t * lock) {
	 // 0 indicates that lock is available, 1 that it is held
		 lock->flag = 0;
		
}

void lock(lock_t * lock) {
	while (TestAndSet(&lock->flag, 1) == 1)
		;// spin-wait (do nothing)
	
}

 void unlock(lock_t * lock) {
	 lock->flag = 0;
}
;----------------------------------------------------Figure 28.2: A Simple Spin Lock Using Test-and-set
Let‚Äôs make sure we understand why this works
1st case:
1. the case where a thread calls lock() and no other thread currently holds the lock; flag should be 0.
   when thread calls TestAndSet(flag, 1), the routine return old flag which is 0; calling thread that (testing) the flag value,
   will not get caught spinning in the while loop and will acquire the lock.
   thread atomically set the value to 1, indicating that the lock is now held.
   finishing with critical section call unlock() to clear the flag = 0;

2nd case:
2. the lock held(i.e., flag is 1). thread in this case, this thread will call TestAndSet(flag, 1)as well.
   and returning the old value with is 1 (cause the lock is held). while simultaneously setting it to 1 again. 
   As long as the lock is held by another thread, TestAndSet() will repeatedly return 1, and thus this
   thread will spin and spin until the lock is finally released. When the flag is finally set to 0 by some other thread, this thread will      call TestAndSet() again, which will now return 0 while atomically setting the value to 1 and thus acquire the lock and enter the       critical section.

By making both the test (of the old lock value) and set (of the new value) a single atomic operation, we ensure that only one thread acquires the lock. how to build a working mutual exclusion primitive!

this type of locks usually referred to as a spin lock. simplest type of lock to build, and
simply spins, using CPU cycles, until the lock becomes available.
To work correctly on a single processor, it requires a preemptive scheduler (i.e., one that will interrupt a thread via a timer, in order to run a different thread, from time to time).

Without preemption, spin locks don‚Äôt make much sense on a single CPU, as a thread spinning on a CPU will never relinquish it.


28.8 Evaluating Spin Locks

given basic spin lock, we can now evaluate how effective it is along our previously described axes.

the most important aspect of a lock is correctness: does it provide mutual exclusion ? the answe here is yes:
the spin lock only one thread allowed to enter the critical section at a time.

the next axis is fairness. How fair is a spin lock to a waiting thread? 
can we make sure that the waiting thread will ever enter the critical section?
bad news : spin locks don't provide any fairness guarantees. thread spinningmay spin forever, under contention. ŸÖŸÖŸÉŸÜ ÿ™ŸÅÿ∂ŸÑ ÿ™ŸÑŸÅ ŸàŸáŸä ŸÑÿ≥Ÿá ÿ®ÿ™ÿ™ÿ¥ŸÉ ÿπŸÑŸä ÿßŸÑŸÅŸÑÿßÿ¨ Ÿàÿ™ÿ∑ÿßŸÑÿ® ÿ®ÿØÿÆŸàŸÑŸáÿß
Spin locks are not fair and may lead to starvation.

The final axis is performance. What are the costs of using a spin lock?
To analyze this more carefully, we suggest thinking about a few different cases.

threads competing for the lock on a signle processor;
threads spread out across many processors.

in single CPU case, performance overhead can be quite painful;
imagin a thread pre-empted within a critical section and held the lock.
the scheduler run every other thread, each of which try to acquire the lock. 
in this case, each will spin for the duration of a time slice before giving up the cpu, a wast of CPU cycles. 


multiple CPUs, spin locks work reasonably well (if the number of threads roughly equals the number of CPUs).
thread A on CPU 1 and thread B on CPU 2, both conteding for a lock. if A grabs the lock, if B tries to, it will spin on CPU 2.
it probably critical section is short, soon lock become available, and acquired by B.
spinning to wait for a lock held on another processor dosn't waste many cycles in this case, and thus can be quite effective.

28.9 Compare-And-Swap

Another hardware primitive that some systems provide is known as the compare-and-swap instruction or compare-and-exchange.
The basic idea is for compare-and-swap to test whether the value at the  address specified by ptr is equal to expected; 
if so, update the memory location pointed to by ptr with the new value. If not, do nothing.

in either case, return the actual value at that memory location, 
thus allowing the code calling compare-and-swap to know whether it succeeded or not.

we can build a lock in a manner quite similar to that with test-and-set.

int CompareAndSwap(int* ptr, int expected, int new) {
	int actual = *ptr;
	if (actual == expected)
		*ptr = new;
	return actual;
}
void lock(lock_t* lock) {
	while (CompareAndSwap(&lock->flag, 0, 1) == 1)
		;// spin
}

This code works quite similarly it simply checks if the flag is 0 and if so, atomically swaps in a 1 thus acquiring the lock.
Threads that try to acquire the lock while it is held will get stuck spinning until the lock is finally released.

Finally, as you may have sensed, compare-and-swap is a more powerful instruction than test-and-set.

why? test-and-set modifies the contents of a memory location and returns its old value as a single atomic operation.

compare-and-swap atomically compares the contents of a memory location to a given value and, only if they are the same, modifies the contents of that memory location to a given new value.

28.10 Load-Linked and Store-Conditional
some platforms provide a pair of instruction that work concert to help build critical section.
for example, the load-linked and store-conditional instructions can be used in tandem Ÿàÿßÿ≠ÿØ Ÿàÿßÿ≠ÿØ to build locks and other concurrent structures.

int LoadLinked(int* ptr) {
	 return *ptr;
	
}
int StoreConditional(int* ptr, int value) {
	if (no one has updated * ptr since the LoadLinked to this address) {
		*ptr = value;
		return 1; // success!

	}
	else {
		return 0; // failed to update

	}

}
/*LESS CODE IS BETTER CODE*/
// short-circuiting boolean conditionals.
//void lock(lock_t* lock) {
//	 while (LoadLinked(&lock->flag) || !StoreConditional(&lock->flag, 1))
//		; // spin
//}
void lock(lock_t* lock) {
	while (1) {
		while (LoadLinked(&lock->flag) == 1)
			; // spin until it‚Äôs zero
		if (StoreConditional(&lock->flag, 1) == 1)
			return; // if set-it-to-1 was a success: all done
		// otherwise: try it all over again

	}

}

 void unlock(lock_t * lock) {
	 lock->flag = 0;
	
}
;---------------------------Using LL/SC To Build A Lock
;-------------------------------------------------------Figure 28.4: Load-linked And Store-conditional-----------------

load-linked operator much like load instruction, simply fetches a value from memory and place it in a register.
key difference come with store-conditional, which only succeeds if no intermittent store to the address has taken place.
in the case of success, store-conditional returns 1 and update value of ptr to value;
in the case of fails, the value ptr in not updated and 0 is returned.

howto build a lock using load-linked and store-conditional.above after modifying,
lock() part, thread spinning waiting to the flag get 0,(the lock in not held).
thus trying acquiring the lock via the store-conditional; if it succeeds, flag set to 1 and thus can proceed into critical section.

Note how failure of the store-conditional might arise!!.
thread call lock() excute load-linked, returning 0 as the lock is not held.
before it can attempt the store-conditional, 
          interrupted and another thread enters the lock code, executing the load-linked instruction, returning also 0 and continuing,
#two threads have each executed the load-linked and each are about to attempt the store-conditional.

üî•The key feature of these instructions is that only one of these threads will succeed in updating the flag to 1 and thus acquire the lock;

the second thread to attempt the store-conditional will fail (because the other thread updated the value of flag between its load-linked and storeconditional) and thus have to try to acquire the lock again.



28.11 Fetch-And-Add
One final hardware primitive is the fetch-and-add instruction, 
which atomically increments a value while returning the old value at a particular address.
we‚Äôll use fetch-and-add to build a more interesting ticket lock,
int FetchAndAdd(int* ptr) {
	 int old = *ptr;
	 * ptr = old + 1;
	 return old;
}
typedef struct __lock_t {
	 int ticket;
	 int turn;
	
} lock_t;

 void lock_init(lock_t * lock) {
	 lock->ticket = 0;
	 lock->turn = 0;
	
}

 void lock(lock_t * lock) {
	 int myturn = FetchAndAdd(&lock->ticket);
	 while (lock->turn != myturn)
		; // spin
	
}

 void unlock(lock_t* lock) {
	 FetchAndAdd(&lock->turn);
 }
instead of a single value, combination of tick and turn value use in this solution.
The basic operation is pretty simple: when thread wishes  to acquire a lock,
if first does an atomic fetch-and-add on the ticket value;
that value is now considered this thread‚Äôs"turn"(myturn).
lock->turn is then used to determine which thread's turn it is; when (myturn == turn) for a given thread,
it is that thread‚Äôs turn to enter the critical section.

Unlock is accomplished simply by incrementing the turn such that the next waiting thread (if
there is one) can now enter the critical section.

Note one important difference with this solution versus our previous attempts: it ensures progress for all threads.
thread is assigned its ticket value, it will be scheduled at some point in the future (once those in
front of it have passed through the critical section and released the lock).
a thread spinning on test-and-set (for example) could spin forever even as other threads acquire and release the lock.


28.12 Summary: So Much Spinning

Our simple hardware-based locks are simple and they work which are two excellent properties of any system or code.
However, in some cases, these solutions can be quite inefficient. 

Imagine you are running two threads on a single processor. 
Now imagine that one thread (thread 0) is in a critical section and thus has a lock held, and unfortunately gets interrupted. 
The second thread (thread 1) now tries to acquire the lock, but finds that it is held. Thus, it begins to spin. And spin.
Then it spins some more. 

And finally, a timer interrupt goes off, thread 0 is run again, which releases the lock, and finally (the next time it runs, say), thread 1 won‚Äôt have to spin so much and will be able to acquire the lock. 

Thus, any time a thread gets caught spinning in a situation like this, it wastes an entire time slice doing nothing but checking a value that isn‚Äôt going to change! The problem gets worse with N threads contending for a lock; N ‚àí 1 time slices may be wasted in a similar manner, simply spinning and waiting for a single thread to release the lock. And thus, our next problem:

HOW TO AVOID SPINNING
How can we develop a lock that doesn‚Äôt needlessly waste time spinning on the CPU? 


28.13 A Simple Approach: Just Yield, Baby
hardware support got us pretty fat: working locks, fainess in lock acquisition.( ticket lock).
but still a problem what to do when context switch occurs in a critical section, thread start spin endlessly, waiting for interrupt on 
run again?

simple approach: when you are going to spin, instead give up the CPU to another thread.
we assume an operating system primitive yield() which a thread can call when it wants to give up the CPU and let another thread run.
because thread can be in one of three states(running, ready, blocked) you can think of OS call that moves the caller from
running state to ready state, and promotes another thread to running.

int TestAndSet(int* ptr, int new) {
	int old = *ptr; // fetch old value at ptr
	*ptr = new; // store ‚Äônew‚Äô into ptr
	return old; // return the old value
}

void init(lock_t * lock) {
	 // 0 indicates that lock is available, 1 that it is held
		 lock->flag = 0;
	
}

void lock(lock_t * lock) {
	while (TestAndSet(&lock->flag, 1) == 1)
		yield();// spin-wait (do nothing), instead give up CPU.
	
}

 void unlock(lock_t * lock) {
	 lock->flag = 0;
}

1st senario:
two threads on one CPU; our yield-based approach works quite well. thread call lock() and found it held, it will simple yield CPU,
other thread will run and finish its critical section.

2nd senario: (worst)
100 threads contending for a lock repeatedly. 
in this case, if one thread acquires the lock and is preempted before releasing it, other 99 will each call lock() ŸÑŸà ÿ≠ÿØ ÿÆÿØ ÿßŸÑŸÇŸÅŸÑ ŸàÿØŸá ŸÖŸäŸÜŸÅÿπÿ¥ Ÿäÿ™ŸÅŸÉ ÿ∫Ÿäÿ± ŸÑŸÖÿß ŸäÿÆŸÑÿµ ÿßŸÑÿ®ÿßŸÇŸä ŸáŸäÿ∑ÿßŸÑÿ®Ÿàÿß ÿ®ÿßŸÑŸÇŸÅŸÑ 
find the lock() held and yield CPU. 

assume some kind of round robin sheduler, each of the 99 will execute this run-and-yield 
pattern before the thread holding the lock gets to run again. 
While better than our spinning approach(which wouldwaste 99 time slices spinning),

this approach is still costly; the cost of a context switch can be substantial, and there is thus plenty of waste.

Worse, we have not tackled the starvation problem at all.
thread may get caught in an endless yield loop while other threads repeatedly enter and exit critical section,
We clearly will need an approach that addresses this problem directly.


28.14 Using Queues: Sleeping Instead Of Spinning

The real problem with our previous approaches is that they leave too much to chance. ŸÖŸÅŸäÿ¥ ÿ≠ÿßÿ¨ÿ© ŸÖÿ™ÿÆÿ∑ÿ∑ ŸÑŸäŸáÿß ŸÉŸÑŸá ÿ®ÿßŸÑÿ≠ÿ®
the scheduler runs next; if the scheduler makes a bad choice, a thread runs must either spin or yield the CPU immediatly.
either way, there is  waste and no prevention of starvation.	

we need more control over who gets the lock next after current holder release it.
to do so we need a little more OS support, as well as queue to keep track of which threads are waiting to enter the lock.

Solaris interface:
for simplicity, we will use park() ÿßÿ±ŸÉŸÜ ÿßŸÜÿ™ ŸÉÿØŸá to put calling thread to sleep, and unpark()(threadID) to wake up thread with particular ID.
there two routines  can be used to build a lock that put caller to sleep if it tries acquire a held lock and wake it when the lock is free.


typedef struct __lock_t {
	 int flag;
	 int guard;
	 queue_t * q;
	
} lock_t;

 void lock_init(lock_t * m) {
	 m->flag = 0;
	 m->guard = 0;
	 queue_init(m->q);
}
int TestAndSet(int* ptr, int new) {
	int old = *ptr; // fetch old value at ptr
	*ptr = new; // store ‚Äônew‚Äô into ptr
	return old; // return the old value
}
 void lock(lock_t * m) {
	 while (TestAndSet(&m->guard, 1) == 1)
		; //acquire guard lock by spinning
	 if (m->flag == 0) {
		 m->flag = 1; // lock is acquired
		 m->guard = 0;
		
	}
	else {
		 queue_add(m->q, gettid());
		 m->guard = 0;
		 park();
		
	}
	
}

 void unlock(lock_t * m) {
	 while (TestAndSet(&m->guard, 1) == 1)
		; //acquire guard lock by spinning
	 if (queue_empty(m->q))
		 m->flag = 0; // let go of lock; no one wants it
	 else
		 unpark(queue_remove(m->q)); // hold lock (for next thread!)
	 m->guard = 0;
	
}

we use test-and-set idea with explicit queue of lock waiters to make a more efficient lock.
we use queue to get control of who gets lock next and thus avoid starvation.


you might Notice section

1Ô∏è‚É£ how the guard is used? 
  spin-lock arount the flag and queue manipulations the lock is using. this approach thus doesn't avoid spin-waiting entirely;
  thread might be interrupted while acquiring or releasing the lock, and thus cause other threads to spin-wait for this one to run again.
However, the time spent spinning is quite limited and thus this approach may be reasonable.

2Ô∏è‚É£ in lock(), when a thread not acquire the lock (it is already held), we add ourselves to a queue(by calling the gettid() to get the thread ID of current thread), set gurd to 0, yield CPU. 

What would happen if the release of the guard lock came after the park(), and not before? endlessly looped.


3Ô∏è‚É£ interesting fact that the flag does not get set back to 0 when another thread gets woken up
  unpard(), m->guard = 0;
when read is workup is return from park(), so it doesn't hold the guard at that point in the code and thus cannot even try to set 
the flag to 1.
we just pass lock directly from thread releasing the lock to the next thread acquiring it; flag is not set 0 in-between.

4Ô∏è‚É£ race condition before the call to park(). with just the wrong timing, a thread will be about to park, assuming that should sleep until 
   the lock is no longer held.
   A switch at that time to another thread could lead to a trouble, if that thread then release the lock. the subsequent park by 
   the first thread would then sleep forever;
called the wakeup/waiting race; to avoid it, we need to do some extra work.

Solaris solves this problemby adding a third systemcall:setpark().
a thread can indicate it is about to park. if it happens to be interrupted and another thread calls unpark before park is actually 
called, the subsequant park returns immediately instead of sleeping.

queue_add(m->q, gettid());
setpark(); // new code
m->guard = 0;

different solution could pass the guard into the kernel. In that case, the kernel could take precautions to atomically release the lock and dequeue the running thread.


28.15 Different OS, Different Support

one type of support that an OS can provide in order to build a more efficient lock in a thread library.

For example, Linux provides something called a futex which is similar to the Solaris interface but provides a bit more in-kernel functionality.
two calls are available.
futex wait(address, expected) puts the calling thread to sleep, assuming the value at address is equal to expected. 
If it is not equal, the call returns immediately.

futex wake(address)wakes one thread that iswaiting on the queue.
basically, it uses a single integer to track both whether the lock is held or not(high bit) and number of waiters on the lock (other bits).

if lock is negative, it is held(high bit is set = 1).
optimize for the common casewhere there is no contention: with only one thread acquiring and releasing a lock, very little work is done.

void mutex_lock(int* mutex) {
	 int v;
	 /* Bit 31 was clear, we got the mutex (this is the fastpath) */
		 if (atomic_bit_test_set(mutex, 31) == 0)
		 return;
	 atomic_increment(mutex);
	 while (1) {
		 if (atomic_bit_test_set(mutex, 31) == 0) {
			 atomic_decrement(mutex);
			 return;
			
		}
		 /* We have to wait now. First make sure the futex value
		 we are monitoring is truly negative (i.e. locked). */
			 v = *mutex;
		 if (v >= 0)
			 continue;
		 futex_wait(mutex, v);
		
	}
	
}

void mutex_unlock(int* mutex) {
	 /* Adding 0x80000000 to the counter results in 0 if and only if
	 there are not other interested threads */
		if (atomic_add_zero(mutex, 0x80000000))
			return;

	/* There are other threads waiting for this mutex,
    wake one of them up. */
	futex_wake(mutex);
}

28.16 Two-Phase Locks

A two-phase lock realizes that spinning can be useful, particularly if the lock is about to be released. 
1Ô∏è‚É£ So in the first phase, the lock spins for a while, hoping that it can acquire the lock.

if the lock is not acquired during the first spin phase, 
2Ô∏è‚É£ a second phase is entered, where the caller is put to sleep, and only woken up when the lock becomes free later.

Linux lock above is a form of such a lock, but it only spins once;

a generalization of this could spin in a loop for a fixed amount of time before using futex support to sleep.

Two-phase locks are yet another instance of a hybrid approach, where combining two good ideas may indeed yield a better one.

Of course,
whether it does depends strongly on many things, including the hardware environment, number of threads, and other workload details. 
As always, making a single general-purpose lock, good for all possible use cases, is quite a challenge.

